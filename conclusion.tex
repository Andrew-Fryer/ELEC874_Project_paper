\section{Conclusions}\label{sec:conclusion}

As mentioned in section \ref{sec:cedar_flaw}, there is at least one significant difference between the genuine and forge images in the CEDAR dataset.
Since CNNs are very powerful and should be able to pick up on this difference very easily, the CEDAR dataset is not a good test of an architecture's ability to understand or verfiy handwritten signatures.

As discussed in section \ref{sec:my_fgsm}, it is evident from figures \ref{dist_vs_perts} and \ref{hist_distances} that perturbations hardly noticable to the human eye are capable of fooling any system that relies on a distance metric.
While training on adversarial examples can improve a model's robustness to them \ref{goodfellow}, it is conjectured that there is no way to make signature verification system that consists of an end-to-end CNN and a threshold function immune to FGSM attacks.
% Applying sufficient successive perturbations results all forges fooling the model. <- I would love to try this and then be able to say it!

The effect of these attacks can be seen in tables \ref{table:1} and \ref{table:2}.
A strange phonemnon appears in the accuracies of the normal vs. adversarial tests.
It is intuitive that the normal accuracy is improved from training for 5 epochs to 20 epochs (espeacially because dropout is used).
Since these accuracies come from the validation set, this does not seem to be from over-fitting.
However, the accuracy computed on the adversarial validation set is worse for the models that have been trained longer.
The cause of this is not known, but I speculate that it is because dropout has increases the linearity of the models.
Dropout encourages the model to build redundant information pathways through the layers.
These pathways might allow the FGSM attack to be more effective.
This seems plausible, espeacially since dropout is known not to ``confer a significant reduction in a model's vulnerability to adversarial examples''\cite{goodfellow}.

As for the dimensionality of the models, it seems that larger latent vectors are slightly better suited to prediction and slightly worse suited to resistence against adversarial attacks.
More tests would be needed to collect statistically significant data.

The impressive accuracies (on the non-adversarial data) do not demonstrate that SigNet is capable performing signature verification accurately.
Likewise, SigNet's 100\% on the CEDAR dataset recorded in the SigNet paper should not be taken too seriously.
First, the CEDAR datset contains differences between the genuine and forged images.
Second, the threshold distance used in the SigNet paper is computed on the validation set, which technically leaks information from the validation set, theoretically allowing the system to perform better than if it were computed on the training set.

In this case, the choice of threshold strategy does not have a significant impact on performance because the data is well seperated (as seen in figure \ref{fig:hist_distances}).
After applying perturbations, the data is not well seperated, but a distance metric alone is not sufficent to seperate it.


% swapping out networks shows/disproves that a black box attack is nearly as effective as a white-box attack.
