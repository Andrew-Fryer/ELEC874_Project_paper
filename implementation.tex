\section{Implementation}\label{sec:implementation}

Take Signet.
Attack it using a technique similar to FGSM.
[Try to improve SigNet's resilience to the attack.]

% TODO: divide this into subsections

For implementing the SigNet, 3 resources were used.
The SigNet paper was consulted, which describes the model completely\cite{sig_net}.
The Keras code presumably by one of the paper authors was used for clarification\cite{GitHub_sounakdey}.
Lastly, an existing PyTorch implementation of SigNet was used as a starting place\cite{GitHub_signet_pytorch}.

The existing PyTorch implementation first restructured into a Jupyter Notebook.
Then, it was checked against the specification of the network in the paper.
The existing training process did not implement data normalization, so this was implemented as per the paper.
The mean and standard deviation of the images in the training dataset was computed (after spliting the images in the CEDAR dataset into training and validation images and pairing images).
For efficiency, all of the images in the dataset were then resized, transformed (inverted), and normalized according the paper's description and then saved back into png files.
[I didn't save the image data to disk as a .pt file (as a tensor) because I figured that png compression is probably better than the .pt compression since it is image-specific.]

Unfortunately, it was later discovered that the last layer of the network was incorrect in the existing implementation and this went unnoticed.
The Signet implementation in this paper has [last layer output size is 256 instead of 128].
% would training go faster if this was fixed?
% I could also try to use the trained incorrect model to initialize the correct model...

The process of preparing the data is all the same.
While the paper uses several datasets, this paper only uses the CEDAR dataset due to time constraints [ref].
% todo: ref

Another small disrepency is in the initialization of model weights.
% todo
% I'm not sure if the biases were correctly initialized to 0 or if the other weights were initialized with the same strategy...

Training was done on Google Colab (using the \_ level tier thing).
The training process took approximately 1 hour per 1000 [cases/datapoints].
The training process was changed to use a dataloader that stores all of the image data in computer memory instead of repeatedly accessing the information from disk in the form of PNG files.
This is feasible because the number of images is far fewer than the number of pairs of images (which are used as [cases/datapoints]).
Unfortunately, this did not seem to speed up the training process significantly.

I should try increasing the batch size!!!!

The model was trained for approximately \_ hours [until batch 19050] and then evaluated, yeilding an accuracy of 72\%.
Then, it was trained until the end of the first epoch (another \_ hours) and evaluated [validated?] again.
This time the accuracy was 94\%.

Although the paper trains the model for 20 epochs, training was stopped at 1 epoch due to time constraints and because 94\% accuracy seemed sufficient for the puposes of this paper.
(Unfortuantely weights from a trained SigNet model could not be obtained.)

% \section{Expirements}
The code in the repo presumably by the SigNet paper author computes accuracy given a thrsehold that is determined by taking the threshold that gives the best accuracy on the validation set.
IMHO, this is wrong because the threshold is information that is technically part of the classifier that is being leaked from the validation set.
See: \url{https://github.com/sounakdey/SigNet/blob/master/SigNet_v1.py#L84}
This approach is useful for understanding the potential of SigNet, but does not give an accuracy evaluation of the accuracy that SigNet would have in the real world because of this information leakage.


I should compute the divide as $p_same / (p_same + p_different)$ where each p is computed as the gaussian distribution of the distances on the training data...

!!! I don't think that SigNet implements the decision (genuine vs. forge) correctly !!!
It treats all dimensions with equal weight, whereas LeCun computes the mean and cov mat of several genuine images and then computes p based on a gaussian/normal assumption.
Yeah, SigNet just finds a threshold distance and uses that... (which naively weights all dimensions equally)

I think that what I should implement (also) is compute the mean and covariance (matices) of the genuine and forge data latent vectors over a bunch of training pairs. (To compute variance, I might need to store the latent vectors in CPU RAM and free the GPU RAM to avoid GPU OOM.)
I can also compute the confusion matrix using the simple threshold distance and my thing and discuss the performance differences.

Maybe I should also try what LeCun did with computing normal distribution for an individual and then using inverse FGSM (aka gradient descent on the input... could I use Pytorch optimizer and stuff for choosing a step value???)

If I transfer the latent vectors to the CPU (and don't include the gradients!), then I can easily store the latent vectors for all of the training images!
(should be 128 floats for each, but FaceNet paper says we can compress to 128 bytes ``without loss of accuracy'' ... but they don't say how exactly...)
Then, I could compute the covariances of the clusters and also do some analytics on understanding the clusters.

Implementing triplet loss for SigNet would be another whole paper I think.
