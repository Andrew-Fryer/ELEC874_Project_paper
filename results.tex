\section{Results}\label{sec:results}

Talk about losses?
Here are the confusion matrices.
Here are some examples of images, pertebations, and resulting perturbed images.
I could also find the n closest signatures to a signature and see what that looks like...
Also try pertebations on swapped networks!
Also, try to perturb non-matching signatures and even random noise...

Here's a table of the accuracy (true/false positive/negative matix).
% todo: compute matrix...

\begin{table*}[t]
    \centering
    \begin{tabular}{|c | c c | c c | >{\bfseries}c | c c >{\em}c|}
    % \begin{tabular}{|m{2.5cm} | m{1cm} | m{1cm} | m{1cm} | m{1cm} | m{1cm} | m{1cm} | m{1cm} | m{1cm}|}
        \hline
        % lvd & tp & fn & fp & tn & acc & fpi & tni & acci\\ [0.1ex]
        % Dimensionality & true positive & false negative & false positive & true negative & accuracy & false positive imp & true negative imp & accuracy imp\\ [0.1ex]
        \multirow{3}{*}{Dimensionality} & \multicolumn{5}{c|}{Normal} & \multicolumn{3}{c|}{Adversarial}\\
        & \multicolumn{2}{c|}{Positive} & \multicolumn{2}{c|}{Negative} & \multirow{2}{*}{Accuracy} & Positive & Negative & \multirow{2}{*}{Accuracy}\\
        % \hline
        & True & False & True & False & & True & False & \\
        \hline
        64  & 82.90 & 17.10 & 0.29 & 99.71 & 91.30 & 32.61 & 67.39 & 83.33\\
        128 & 75.65 & 24.35 & 16.23 & 83.77 & 79.71 & 87.75 & 12.25 & 57.22\\
        256 & 52.83 & 47.17 & 6.38 & 93.62 & 73.22 & 27.32 & 72.68 & 73.04\\ [0.1ex]
        \hline
    \end{tabular}
    \caption{Comparison of Accuracy using Latent Vector Sizes}
    \label{table:1}
\end{table*}

Here's a genuine and a forge and we apply various epsilon values and also repeat...
    There are diminising returns on continually nudging a forge.

Here's how well we trick SigNet...

Here's the same thing with 2 genuines of different signatures.

Here's the same thing with 2 genuines of the same signature.


the training loss is not stable
the accuracy after \_ amount of training was 72% accuracy
It would take too long to get 100% accuracy

The validation loss was higher than the training loss even on the 1st epoch of training.
I believe this is because the images in the datapoints (image pairs with labels) have been seen before...


random noise can score well...

-------------------------------------------------

There are pure 0s in the forge, but not the original...
what if I add a bit of random noise to the forge?

I wonder if making the pixels boolean would help or hurt signet/advarsary...





WAT
    scaling the input to the net doesn't seem to change its output (latent) vector
    It that because it is doing some sort of normalization??
